{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.holoviz.org/panel/1.4.4/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
            "application/vnd.holoviews_load.v0+json": ""
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
            "application/vnd.holoviews_load.v0+json": ""
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>*[data-root-id],\n",
              "*[data-root-id] > * {\n",
              "  box-sizing: border-box;\n",
              "  font-family: var(--jp-ui-font-family);\n",
              "  font-size: var(--jp-ui-font-size1);\n",
              "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
              "}\n",
              "\n",
              "/* Override VSCode background color */\n",
              ".cell-output-ipywidget-background:has(\n",
              "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
              "  ),\n",
              ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
              "  background-color: transparent !important;\n",
              "}\n",
              "</style>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.holoviews_exec.v0+json": "",
            "text/html": [
              "<div id='p1002'>\n",
              "  <div id=\"d45e993e-fa9c-4092-8990-40d1b001d4a2\" data-root-id=\"p1002\" style=\"display: contents;\"></div>\n",
              "</div>\n",
              "<script type=\"application/javascript\">(function(root) {\n",
              "  var docs_json = {\"76bf1d14-0c04-4379-b5e9-246757432530\":{\"version\":\"3.4.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"p1002\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"p1003\",\"attributes\":{\"plot_id\":\"p1002\",\"comm_id\":\"e8ea6ff7e51a42118535bb208416b360\",\"client_comm_id\":\"eaa81d41752647879020a4933103410a\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
              "  var render_items = [{\"docid\":\"76bf1d14-0c04-4379-b5e9-246757432530\",\"roots\":{\"p1002\":\"d45e993e-fa9c-4092-8990-40d1b001d4a2\"},\"root_ids\":[\"p1002\"]}];\n",
              "  var docs = Object.values(docs_json)\n",
              "  if (!docs) {\n",
              "    return\n",
              "  }\n",
              "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
              "  async function embed_document(root) {\n",
              "    var Bokeh = get_bokeh(root)\n",
              "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
              "    for (const render_item of render_items) {\n",
              "      for (const root_id of render_item.root_ids) {\n",
              "\tconst id_el = document.getElementById(root_id)\n",
              "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
              "\t  const root_el = id_el.children[0]\n",
              "\t  root_el.id = root_el.id + '-rendered'\n",
              "\t  for (const child of root_el.children) {\n",
              "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
              "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
              "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
              "\t  }\n",
              "\t}\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "  function get_bokeh(root) {\n",
              "    if (root.Bokeh === undefined) {\n",
              "      return null\n",
              "    } else if (root.Bokeh.version !== py_version) {\n",
              "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
              "\treturn null\n",
              "      }\n",
              "      return root.Bokeh.versions.get(py_version);\n",
              "    } else if (root.Bokeh.version === py_version) {\n",
              "      return root.Bokeh\n",
              "    }\n",
              "    return null\n",
              "  }\n",
              "  function is_loaded(root) {\n",
              "    var Bokeh = get_bokeh(root)\n",
              "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
              "  }\n",
              "  if (is_loaded(root)) {\n",
              "    embed_document(root);\n",
              "  } else {\n",
              "    var attempts = 0;\n",
              "    var timer = setInterval(function(root) {\n",
              "      if (is_loaded(root)) {\n",
              "        clearInterval(timer);\n",
              "        embed_document(root);\n",
              "      } else if (document.readyState == \"complete\") {\n",
              "        attempts++;\n",
              "        if (attempts > 200) {\n",
              "          clearInterval(timer);\n",
              "\t  var Bokeh = get_bokeh(root)\n",
              "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
              "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
              "\t  } else {\n",
              "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
              "\t    embed_document(root)\n",
              "\t  }\n",
              "        }\n",
              "      }\n",
              "    }, 25, root)\n",
              "  }\n",
              "})(window);</script>"
            ]
          },
          "metadata": {
            "application/vnd.holoviews_exec.v0+json": {
              "id": "p1002"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#IMPORTING AND DOWNLOADING ALL THE REQUIRED MEANS\n",
        "\n",
        "# !pip install -q hvplot\n",
        "# !pip install scikit-learn==1.2.2\n",
        "# !pip install imblearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import hvplot\n",
        "import hvplot.pandas\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
            "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
            "1   49        No  Travel_Frequently        279  Research & Development   \n",
            "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
            "3   33        No  Travel_Frequently       1392  Research & Development   \n",
            "4   27        No      Travel_Rarely        591  Research & Development   \n",
            "\n",
            "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
            "0                 1          2  Life Sciences              1               1   \n",
            "1                 8          1  Life Sciences              1               2   \n",
            "2                 2          2          Other              1               4   \n",
            "3                 3          4  Life Sciences              1               5   \n",
            "4                 2          1        Medical              1               7   \n",
            "\n",
            "   ...  RelationshipSatisfaction StandardHours  StockOptionLevel  \\\n",
            "0  ...                         1            80                 0   \n",
            "1  ...                         4            80                 1   \n",
            "2  ...                         2            80                 0   \n",
            "3  ...                         3            80                 0   \n",
            "4  ...                         4            80                 1   \n",
            "\n",
            "   TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
            "0                  8                      0               1               6   \n",
            "1                 10                      3               3              10   \n",
            "2                  7                      3               3               0   \n",
            "3                  8                      3               3               8   \n",
            "4                  6                      3               3               2   \n",
            "\n",
            "  YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
            "0                  4                        0                     5  \n",
            "1                  7                        1                     7  \n",
            "2                  0                        0                     0  \n",
            "3                  7                        3                     0  \n",
            "4                  2                        2                     2  \n",
            "\n",
            "[5 rows x 35 columns]\n"
          ]
        }
      ],
      "source": [
        "#READING FILE FROM THE SYSTEM\n",
        "\n",
        "file_path = r\"C:\\Users\\Suyash Shringi\\OneDrive\\Desktop\\Projects\\Employee Retention\\WA_Fn-UseC_-HR-Employee-Attrition.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #CHECKING NUMBER OF UNIQUE VALUES FOR EVERY FEATURE\n",
        "\n",
        "# for column in df.columns:\n",
        "#     print(f\"{column}: {df[column].nunique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#REMOVING CONSTANT FEATURES ,i.e., EmployeeCount, Over18, StandardHours have 1 unique value and EmployeeNumber is irrelevant to the O/P of the model\n",
        "\n",
        "df.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], axis=\"columns\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #CHECKING DIMENSIONS OF THE DATASET\n",
        "\n",
        "# df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CONVERTING O/P CATEGORICAL VARIABLE TO THE NUMERICAL FEATURE OF FORM 1/0(YES/NO)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "output_Data = LabelEncoder()\n",
        "df[\"Attrition\"] = output_Data.fit_transform(df.Attrition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Age                         0\n",
              "Attrition                   0\n",
              "BusinessTravel              0\n",
              "DailyRate                   0\n",
              "Department                  0\n",
              "DistanceFromHome            0\n",
              "Education                   0\n",
              "EducationField              0\n",
              "EnvironmentSatisfaction     0\n",
              "Gender                      0\n",
              "HourlyRate                  0\n",
              "JobInvolvement              0\n",
              "JobLevel                    0\n",
              "JobRole                     0\n",
              "JobSatisfaction             0\n",
              "MaritalStatus               0\n",
              "MonthlyIncome               0\n",
              "MonthlyRate                 0\n",
              "NumCompaniesWorked          0\n",
              "OverTime                    0\n",
              "PercentSalaryHike           0\n",
              "PerformanceRating           0\n",
              "RelationshipSatisfaction    0\n",
              "StockOptionLevel            0\n",
              "TotalWorkingYears           0\n",
              "TrainingTimesLastYear       0\n",
              "WorkLifeBalance             0\n",
              "YearsAtCompany              0\n",
              "YearsInCurrentRole          0\n",
              "YearsSinceLastPromotion     0\n",
              "YearsWithCurrManager        0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#CHECKING IF ANY MISSING VALUES OR PRESENT OR NOT\n",
        "\n",
        "df.isna().sum()\n",
        "\n",
        "#NO MISSING VALUES ARE PRESENT IN ANY FEATURE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#CHECKING IF ANY DUPLICATE VALUE IS PRESENT OR NOT\n",
        "\n",
        "df.duplicated().sum()\n",
        "\n",
        "#NO DUPLICATES ARE PRESENT IN THE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OverTime\n",
              "No     1054\n",
              "Yes     416\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['OverTime'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CONVERTING ALL CATEGORICAL VARIABLES TO NUMERICAL FEATURE, USING ORDINAL ENCODER\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "enc=OrdinalEncoder()\n",
        "enc.fit(df[[\"BusinessTravel\",\"Department\", \"EducationField\", \"Gender\", \"JobRole\", \"MaritalStatus\",  \"OverTime\"]])\n",
        "df[[\"BusinessTravel\",\"Department\", \"EducationField\", \"Gender\", \"JobRole\", \"MaritalStatus\",  \"OverTime\"]] = enc.transform(df[[\"BusinessTravel\",\"Department\", \"EducationField\", \"Gender\", \"JobRole\", \"MaritalStatus\",  \"OverTime\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OverTime\n",
              "0.0    1054\n",
              "1.0     416\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['OverTime'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #PLOTTING BOX-PLOT TO CHECK FOR OUTLIERS\n",
        "\n",
        "# ColsBox = df.select_dtypes('number')\n",
        "# for col in ColsBox.columns:\n",
        "#     plt.figure(figsize=(10,6))\n",
        "#     plt.title('box plot of '+col)\n",
        "#     sns.boxplot(df[col])\n",
        "#     plt.show()\n",
        "\n",
        "# #OBSERVATION :\n",
        "# #1. MonthlyIncome(+)\n",
        "# #2. NumCompaniesWorked(+)\n",
        "# #3. StockOptionLevel(+)\n",
        "# #4. TotalWorkingYears(+)\n",
        "# #5. TrainingThisLastYear(+/-)\n",
        "# #6. YearsAtCompany(+)\n",
        "# #7. YearsInCurrentRole(+)\n",
        "# #8. YearsSinceLastPromtion(+)\n",
        "# #9. YearsWithCurrentManager(+)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "#DESIGNING FUNCTION TO REMOVE OUTLIERS USING THE CAPPING METHOD, cause WHILE TRIMMING, EXCESSIVE DATA GETS REMOVED\n",
        "\n",
        "def remove_outliers(df):\n",
        "    for col in df.select_dtypes(include='number').columns:\n",
        "        if col == 'PerformanceRating':\n",
        "            continue\n",
        "        if col == 'Attrition':\n",
        "            continue\n",
        "        percentile25 = df[col].quantile(0.25)\n",
        "        percentile75 = df[col].quantile(0.75)\n",
        "        iqr = percentile75 - percentile25\n",
        "        print(iqr)\n",
        "        upper_limit = percentile75 + 1.5 * iqr\n",
        "        lower_limit = percentile25 - 1.5 * iqr\n",
        "        print(upper_limit)\n",
        "        print(lower_limit)\n",
        "        df[col] = np.where(df[col] > upper_limit, upper_limit, df[col])\n",
        "        df[col] = np.where(df[col] <lower_limit, lower_limit, df[col])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13.0\n",
            "62.5\n",
            "10.5\n",
            "1.0\n",
            "3.5\n",
            "-0.5\n",
            "692.0\n",
            "2195.0\n",
            "-573.0\n",
            "1.0\n",
            "3.5\n",
            "-0.5\n",
            "12.0\n",
            "32.0\n",
            "-16.0\n",
            "2.0\n",
            "7.0\n",
            "-1.0\n",
            "2.0\n",
            "6.0\n",
            "-2.0\n",
            "2.0\n",
            "7.0\n",
            "-1.0\n",
            "1.0\n",
            "2.5\n",
            "-1.5\n",
            "35.75\n",
            "137.375\n",
            "-5.625\n",
            "1.0\n",
            "4.5\n",
            "0.5\n",
            "2.0\n",
            "6.0\n",
            "-2.0\n",
            "5.0\n",
            "14.5\n",
            "-5.5\n",
            "2.0\n",
            "7.0\n",
            "-1.0\n",
            "1.0\n",
            "3.5\n",
            "-0.5\n",
            "5468.0\n",
            "16581.0\n",
            "-5291.0\n",
            "12414.5\n",
            "39083.25\n",
            "-10574.75\n",
            "3.0\n",
            "8.5\n",
            "-3.5\n",
            "1.0\n",
            "2.5\n",
            "-1.5\n",
            "6.0\n",
            "27.0\n",
            "3.0\n",
            "2.0\n",
            "7.0\n",
            "-1.0\n",
            "1.0\n",
            "2.5\n",
            "-1.5\n",
            "9.0\n",
            "28.5\n",
            "-7.5\n",
            "1.0\n",
            "4.5\n",
            "0.5\n",
            "1.0\n",
            "4.5\n",
            "0.5\n",
            "6.0\n",
            "18.0\n",
            "-6.0\n",
            "5.0\n",
            "14.5\n",
            "-5.5\n",
            "3.0\n",
            "7.5\n",
            "-4.5\n",
            "5.0\n",
            "14.5\n",
            "-5.5\n"
          ]
        }
      ],
      "source": [
        "# Remove outliers\n",
        "df = remove_outliers(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #RE-CHECKING TO ENSURE NO MORE OUTLIERS ARE PRESENT\n",
        "\n",
        "# ColsBox = df.select_dtypes('number')\n",
        "# for col in ColsBox.columns:\n",
        "#     plt.figure(figsize=(10,6))\n",
        "#     plt.title('box plot of '+col)\n",
        "#     sns.boxplot(df[col])\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #PLOTTING HISTOGRAMS TO UNDERSTAND THE DISTRIBUTION OF THE FEATURE\n",
        "\n",
        "# histplotter = df.select_dtypes('number')\n",
        "# for col in histplotter.columns:\n",
        "#     plt.figure(figsize=(10,6))\n",
        "#     plt.title('HIST '+col)\n",
        "#     sns.histplot(data=df, x=col, hue='Attrition', bins=30, element='step')\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "#PERFORMING TRAIN/TEST SPLIT\n",
        "\n",
        "X = df.drop('Attrition', axis=1)\n",
        "y = df['Attrition']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "#REMOVING COLUMNS THAT ARE HIGHELY CO-RELATED\n",
        "# 1. Monthly Income - Job Level (0.94)\n",
        "# 2. Years in current role - Years at company (0.82)\n",
        "# 3. Years at company - Years with currentÂ manager(0.84)\n",
        "\n",
        "# DROPPING THE ONE FEATURE WHICH HAS LESS IMPACT ON THE ACCURACY AND F1 SCORE\n",
        "\n",
        "X_train.drop(['MonthlyIncome', 'YearsAtCompany', 'YearsInCurrentRole'] , axis=1, inplace=True)\n",
        "X_test.drop(['MonthlyIncome', 'YearsAtCompany', 'YearsInCurrentRole'] , axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#DROPPING FEATURES, BASED ON THEIR POOR PERFORMANCE ON ALL THE ABOVE TESTS CONDUCTED\n",
        "\n",
        "X_train.drop(['PerformanceRating', 'RelationshipSatisfaction', 'DistanceFromHome', 'JobInvolvement', 'JobSatisfaction', 'EnvironmentSatisfaction','TrainingTimesLastYear', 'Gender'] , axis=1, inplace=True)\n",
        "X_test.drop(['PerformanceRating', 'RelationshipSatisfaction', 'DistanceFromHome', 'JobInvolvement', 'JobSatisfaction', 'EnvironmentSatisfaction','TrainingTimesLastYear', 'Gender'] , axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Age  DailyRate  Education  HourlyRate  JobLevel  MonthlyRate  \\\n",
            "0 -0.426594   0.073582   1.034871   -0.138483 -0.966574     0.056416   \n",
            "1  0.009360   1.517771  -0.892300    0.887919 -0.966574     1.562622   \n",
            "2  1.426212   1.005718  -1.855885    0.399156 -0.966574    -1.560354   \n",
            "3  0.118349  -1.457607   0.071286   -1.067131  0.855047    -0.787110   \n",
            "4  1.317223  -0.684556  -0.892300   -0.724998  0.855047     1.141070   \n",
            "\n",
            "   NumCompaniesWorked  PercentSalaryHike  StockOptionLevel  TotalWorkingYears  \\\n",
            "0           -0.681533          -0.620119          0.311165          -1.251429   \n",
            "1           -1.090558          -0.348311         -0.984162          -0.561665   \n",
            "2           -0.681533          -1.163734          2.254156           1.231721   \n",
            "3           -1.090558           1.554342         -0.984162          -0.147807   \n",
            "4            0.545540          -1.163734          0.311165           2.404320   \n",
            "\n",
            "   WorkLifeBalance  YearsSinceLastPromotion  YearsWithCurrManager  \\\n",
            "0        -1.056766                -0.764810             -0.602212   \n",
            "1        -2.467387                -0.367541             -0.318762   \n",
            "2         0.353855                 0.426996              1.098492   \n",
            "3         0.353855                -0.367541              1.098492   \n",
            "4        -1.056766                -0.367541              0.815041   \n",
            "\n",
            "   BusinessTravel  Department  EducationField  JobRole  MaritalStatus  \\\n",
            "0             2.0         1.0             1.0      6.0            1.0   \n",
            "1             0.0         1.0             5.0      2.0            2.0   \n",
            "2             2.0         1.0             3.0      2.0            1.0   \n",
            "3             1.0         1.0             1.0      4.0            2.0   \n",
            "4             2.0         1.0             4.0      2.0            1.0   \n",
            "\n",
            "   OverTime  \n",
            "0       0.0  \n",
            "1       0.0  \n",
            "2       1.0  \n",
            "3       0.0  \n",
            "4       0.0  \n",
            "        Age  DailyRate  Education  HourlyRate  JobLevel  MonthlyRate  \\\n",
            "0 -1.952434  -0.741727   0.071286    1.474433 -0.966574    -1.158802   \n",
            "1 -0.099628   1.651999  -0.892300    1.278928  0.855047     1.207972   \n",
            "2  1.971155  -1.330837   1.034871    0.936795  1.765857    -0.718096   \n",
            "3 -0.426594  -1.020125   1.034871    1.327805 -0.055764    -1.472466   \n",
            "4  0.445315   1.386029   0.071286   -0.822750  2.676668     0.268248   \n",
            "\n",
            "   NumCompaniesWorked  PercentSalaryHike  StockOptionLevel  TotalWorkingYears  \\\n",
            "0           -0.681533           2.641573         -0.984162          -1.389382   \n",
            "1           -1.090558          -0.348311          0.311165          -0.147807   \n",
            "2            1.363588           0.467112         -0.984162           1.783533   \n",
            "3           -0.681533          -0.348311          1.606492          -0.699618   \n",
            "4           -0.681533          -0.891927         -0.984162           1.645580   \n",
            "\n",
            "   WorkLifeBalance  YearsSinceLastPromotion  YearsWithCurrManager  \\\n",
            "0         1.764475                -0.764810             -1.169114   \n",
            "1         0.353855                 0.029728              1.098492   \n",
            "2        -1.056766                 0.426996              1.098492   \n",
            "3         0.353855                -0.764810             -0.035311   \n",
            "4         0.353855                 2.214705              1.098492   \n",
            "\n",
            "   BusinessTravel  Department  EducationField  JobRole  MaritalStatus  \\\n",
            "0             0.0         1.0             3.0      6.0            2.0   \n",
            "1             1.0         2.0             5.0      7.0            1.0   \n",
            "2             2.0         2.0             2.0      7.0            2.0   \n",
            "3             2.0         2.0             3.0      7.0            0.0   \n",
            "4             2.0         1.0             5.0      5.0            1.0   \n",
            "\n",
            "   OverTime  \n",
            "0       1.0  \n",
            "1       0.0  \n",
            "2       1.0  \n",
            "3       0.0  \n",
            "4       0.0  \n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# List of categorical columns\n",
        "categorical_columns = [\n",
        "    'BusinessTravel', 'Department', 'EducationField',\n",
        "    'JobRole', 'MaritalStatus', 'OverTime'\n",
        "]\n",
        "\n",
        "\n",
        "# Separate numerical and categorical columns\n",
        "X_train_num = X_train.drop(columns=categorical_columns)\n",
        "X_test_num = X_test.drop(columns=categorical_columns)\n",
        "\n",
        "X_train_cat = X_train[categorical_columns]\n",
        "X_test_cat = X_test[categorical_columns]\n",
        "\n",
        "# Apply scaling to the numerical columns\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_num = scaler.fit_transform(X_train_num)\n",
        "X_test_scaled_num = scaler.transform(X_test_num)\n",
        "\n",
        "# Transform back into DataFrame\n",
        "X_train_scaled_num = pd.DataFrame(X_train_scaled_num, columns=X_train_num.columns)\n",
        "X_test_scaled_num = pd.DataFrame(X_test_scaled_num, columns=X_test_num.columns)\n",
        "\n",
        "# Concatenate scaled numerical data with categorical data\n",
        "X_train_scaled = pd.concat([X_train_scaled_num, X_train_cat.reset_index(drop=True)], axis=1)\n",
        "X_test_scaled = pd.concat([X_test_scaled_num, X_test_cat.reset_index(drop=True)], axis=1)\n",
        "\n",
        "X_train=X_train_scaled\n",
        "X_test=X_test_scaled\n",
        "\n",
        "# Check the results\n",
        "print(X_train.head())\n",
        "print(X_test.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "#IMPORTING LIBRARIES TO CHECK INTITAL ACCURACY\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score,confusion_matrix, classification_report, mean_squared_error\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "#FUNCTION TO COMPUTE ACCURACY_SCORE, CONFUSION_MATRIX, \n",
        "\n",
        "def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n",
        "    \n",
        "    if train:\n",
        "        pred = clf.predict(X_train)\n",
        "        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n",
        "        print(\"Train Result:\\n\")\n",
        "        print(f\"Confusion Matrix: \\n{confusion_matrix(y_train, pred)}\\n\")\n",
        "        print(f\"Mean Squared Error: {mean_squared_error(y_train, pred):.2f}\\n\")\n",
        "        print(f\"Classification Report: \\n{clf_report}\\n\")\n",
        "        print(f\"Precision: {clf_report.loc['precision', 'weighted avg']:.2f}\")\n",
        "        print(f\"Recall: {clf_report.loc['recall', 'weighted avg']:.2f}\")\n",
        "        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")        \n",
        "\n",
        "    \n",
        "    else:\n",
        "        pred = clf.predict(X_test)\n",
        "        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n",
        "        print(\"Test Result:\\n\")        \n",
        "        print(f\"Confusion Matrix: \\n{confusion_matrix(y_test, pred)}\\n\")\n",
        "        print(f\"Mean Squared Error: {mean_squared_error(y_test, pred):.2f}\\n\")\n",
        "        print(f\"Classification Report: \\n{clf_report}\\n\")\n",
        "        print(f\"Precision: {clf_report.loc['precision', 'weighted avg']:.2f}\")\n",
        "        print(f\"Recall: {clf_report.loc['recall', 'weighted avg']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "#FUNCTION TO COMPUTE ACCURACY_SCORE, CONFUSION_MATRIX, MEAN_SQUARED_ERROR\n",
        "\n",
        "def print_score1(clf, X_train, y_train, X_test, y_test, train=True):\n",
        "    \n",
        "    if train:\n",
        "        pred = clf.predict(X_train)\n",
        "        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n",
        "        print(\"Train Result:\\n\")\n",
        "        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")        \n",
        "        print(f\"Confusion Matrix: \\n{confusion_matrix(y_train, pred)}\\n\")\n",
        "        print(f\"Mean Squared Error: {mean_squared_error(y_train, pred):.2f}\\n\")\n",
        "        print(f\"Classification Report: \\n{clf_report}\\n\")\n",
        "        print(f\"Precision: {clf_report.loc['precision', 'weighted avg']:.2f}\")\n",
        "        print(f\"Recall: {clf_report.loc['recall', 'weighted avg']:.2f}\")\n",
        "    \n",
        "    else:\n",
        "        pred = clf.predict(X_test)\n",
        "        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n",
        "        print(\"Test Result:\\n\")        \n",
        "        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")        \n",
        "        print(f\"Confusion Matrix: \\n{confusion_matrix(y_test, pred)}\\n\")\n",
        "        print(f\"Mean Squared Error: {mean_squared_error(y_test, pred):.2f}\\n\")\n",
        "        print(f\"Classification Report: \\n{clf_report}\\n\")\n",
        "        print(f\"Precision: {clf_report.loc['precision', 'weighted avg']:.2f}\")\n",
        "        print(f\"Recall: {clf_report.loc['recall', 'weighted avg']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------\n",
            "\n",
            "Classifier: Decision Tree\n",
            "Train Result:\n",
            "\n",
            "Accuracy Score: 100.00%\n",
            "Confusion Matrix: \n",
            "[[989   0]\n",
            " [  0 187]]\n",
            "\n",
            "Mean Squared Error: 0.00\n",
            "\n",
            "Classification Report: \n",
            "               0      1  accuracy  macro avg  weighted avg\n",
            "precision    1.0    1.0       1.0        1.0           1.0\n",
            "recall       1.0    1.0       1.0        1.0           1.0\n",
            "f1-score     1.0    1.0       1.0        1.0           1.0\n",
            "support    989.0  187.0       1.0     1176.0        1176.0\n",
            "\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "Test Result:\n",
            "\n",
            "Accuracy Score: 75.51%\n",
            "Confusion Matrix: \n",
            "[[207  37]\n",
            " [ 35  15]]\n",
            "\n",
            "Mean Squared Error: 0.24\n",
            "\n",
            "Classification Report: \n",
            "                    0          1  accuracy   macro avg  weighted avg\n",
            "precision    0.855372   0.288462  0.755102    0.571917      0.758959\n",
            "recall       0.848361   0.300000  0.755102    0.574180      0.755102\n",
            "f1-score     0.851852   0.294118  0.755102    0.572985      0.756999\n",
            "support    244.000000  50.000000  0.755102  294.000000    294.000000\n",
            "\n",
            "Precision: 0.76\n",
            "Recall: 0.76\n",
            "---------------------------\n",
            "\n",
            "---------------------------\n",
            "\n",
            "Classifier: Random Forest\n",
            "Train Result:\n",
            "\n",
            "Accuracy Score: 100.00%\n",
            "Confusion Matrix: \n",
            "[[989   0]\n",
            " [  0 187]]\n",
            "\n",
            "Mean Squared Error: 0.00\n",
            "\n",
            "Classification Report: \n",
            "               0      1  accuracy  macro avg  weighted avg\n",
            "precision    1.0    1.0       1.0        1.0           1.0\n",
            "recall       1.0    1.0       1.0        1.0           1.0\n",
            "f1-score     1.0    1.0       1.0        1.0           1.0\n",
            "support    989.0  187.0       1.0     1176.0        1176.0\n",
            "\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "Test Result:\n",
            "\n",
            "Accuracy Score: 85.03%\n",
            "Confusion Matrix: \n",
            "[[242   2]\n",
            " [ 42   8]]\n",
            "\n",
            "Mean Squared Error: 0.15\n",
            "\n",
            "Classification Report: \n",
            "                    0          1  accuracy   macro avg  weighted avg\n",
            "precision    0.852113   0.800000   0.85034    0.826056      0.843250\n",
            "recall       0.991803   0.160000   0.85034    0.575902      0.850340\n",
            "f1-score     0.916667   0.266667   0.85034    0.591667      0.806122\n",
            "support    244.000000  50.000000   0.85034  294.000000    294.000000\n",
            "\n",
            "Precision: 0.84\n",
            "Recall: 0.85\n",
            "---------------------------\n",
            "\n",
            "---------------------------\n",
            "\n",
            "Classifier: XGB\n",
            "Train Result:\n",
            "\n",
            "Accuracy Score: 100.00%\n",
            "Confusion Matrix: \n",
            "[[989   0]\n",
            " [  0 187]]\n",
            "\n",
            "Mean Squared Error: 0.00\n",
            "\n",
            "Classification Report: \n",
            "               0      1  accuracy  macro avg  weighted avg\n",
            "precision    1.0    1.0       1.0        1.0           1.0\n",
            "recall       1.0    1.0       1.0        1.0           1.0\n",
            "f1-score     1.0    1.0       1.0        1.0           1.0\n",
            "support    989.0  187.0       1.0     1176.0        1176.0\n",
            "\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "Test Result:\n",
            "\n",
            "Accuracy Score: 82.99%\n",
            "Confusion Matrix: \n",
            "[[232  12]\n",
            " [ 38  12]]\n",
            "\n",
            "Mean Squared Error: 0.17\n",
            "\n",
            "Classification Report: \n",
            "                    0          1  accuracy   macro avg  weighted avg\n",
            "precision    0.859259   0.500000  0.829932    0.679630      0.798161\n",
            "recall       0.950820   0.240000  0.829932    0.595410      0.829932\n",
            "f1-score     0.902724   0.324324  0.829932    0.613524      0.804356\n",
            "support    244.000000  50.000000  0.829932  294.000000    294.000000\n",
            "\n",
            "Precision: 0.80\n",
            "Recall: 0.83\n",
            "---------------------------\n",
            "\n",
            "---------------------------\n",
            "\n",
            "Classifier: Naive Bayes\n",
            "Train Result:\n",
            "\n",
            "Accuracy Score: 83.08%\n",
            "Confusion Matrix: \n",
            "[[889 100]\n",
            " [ 99  88]]\n",
            "\n",
            "Mean Squared Error: 0.17\n",
            "\n",
            "Classification Report: \n",
            "                    0           1  accuracy    macro avg  weighted avg\n",
            "precision    0.899798    0.468085  0.830782     0.683941      0.831149\n",
            "recall       0.898888    0.470588  0.830782     0.684738      0.830782\n",
            "f1-score     0.899342    0.469333  0.830782     0.684338      0.830965\n",
            "support    989.000000  187.000000  0.830782  1176.000000   1176.000000\n",
            "\n",
            "Precision: 0.83\n",
            "Recall: 0.83\n",
            "Test Result:\n",
            "\n",
            "Accuracy Score: 83.33%\n",
            "Confusion Matrix: \n",
            "[[226  18]\n",
            " [ 31  19]]\n",
            "\n",
            "Mean Squared Error: 0.17\n",
            "\n",
            "Classification Report: \n",
            "                    0          1  accuracy   macro avg  weighted avg\n",
            "precision    0.879377   0.513514  0.833333    0.696445      0.817156\n",
            "recall       0.926230   0.380000  0.833333    0.653115      0.833333\n",
            "f1-score     0.902196   0.436782  0.833333    0.669489      0.823044\n",
            "support    244.000000  50.000000  0.833333  294.000000    294.000000\n",
            "\n",
            "Precision: 0.82\n",
            "Recall: 0.83\n",
            "---------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#CHECKING VARIOUS METRICS ACROSS DIFFERENT MODELS\n",
        "\n",
        "classifiers = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"XGB\": XGBClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB()\n",
        "}\n",
        "for clf_name, clf in classifiers.items():\n",
        "    clf.fit(X_train_scaled, y_train)\n",
        "    print(\"---------------------------\\n\")\n",
        "    print(f\"Classifier: {clf_name}\")\n",
        "    print_score1(clf, X_train_scaled, y_train, X_test_scaled, y_test, train=True)\n",
        "    print_score1(clf, X_train_scaled, y_train, X_test_scaled, y_test, train=False) \n",
        "    print(\"---------------------------\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #CREATING CO-RELATION MATRIX\n",
        "\n",
        "# plt.figure(figsize=(19,13))\n",
        "# cor=X_train.corr()\n",
        "# sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #CO-RELATION FUNCTINO TO CHECK FOR FEATURES THAT ARE ABOVE THE THRESHOLD\n",
        "\n",
        "# def correlation(dataset, threshold):\n",
        "#     col_corr = set() # Set of all the names of deleted columns\n",
        "#     corr_matrix = dataset.corr()\n",
        "#     for i in range(len(corr_matrix.columns)):\n",
        "#         for j in range(i):\n",
        "#             if ((corr_matrix.iloc[i, j] >= threshold)):\n",
        "#                 colname = corr_matrix.columns[i] # getting the name of column\n",
        "#                 col_corr.add(colname)\n",
        "\n",
        "#     return col_corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #SETTING THE THRESHOLD VALUE, WHICH IS IN THIS 0.8\n",
        "\n",
        "# col_corr = correlation(X_train, 0.8)\n",
        "# print(col_corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "collapsed": true,
        "id": "9i6Gy1VsbwDD",
        "outputId": "a042dc7c-45be-45db-c32d-8a21cb4b1108"
      },
      "outputs": [],
      "source": [
        "# #IMPLEMENTING CHI-SQUARE TEST\n",
        "\n",
        "# from sklearn.feature_selection import chi2\n",
        "# values = chi2(X_train, y_train)\n",
        "\n",
        "# f_values=pd.Series(values[0])\n",
        "# f_values.index = X_train.columns\n",
        "# f_values.sort_values(ascending=False, inplace=True)\n",
        "# p_values=pd.Series(values[1])\n",
        "# p_values.index = X_train.columns\n",
        "# p_values.sort_values(ascending=True, inplace=True)\n",
        "\n",
        "# #OBSERVATION :\n",
        "# # 1. OVERTIME IS THE MOST IMPORTANT COLUMN\n",
        "# # 2. Whereas hourly rate, performance rating, business travel are least important column in that order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "PEaYv_QtbuKl"
      },
      "outputs": [],
      "source": [
        "#HIGHER THE F_VALUE HIGHER IS THE IMPORTANCE\n",
        "\n",
        "# f_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eX1RBDLweI0_",
        "outputId": "b85b13f0-1ccc-485b-f2d3-83fd32978a65"
      },
      "outputs": [],
      "source": [
        "#LOWER P_VALUE INDICATES HIGHER PRIORITY FEATURE\n",
        "\n",
        "# p_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ogCql3EzwEH-"
      },
      "outputs": [],
      "source": [
        "# #INFO GAIN FeatureSelctionTechnique\n",
        "\n",
        "# from sklearn.feature_selection import mutual_info_classif\n",
        "# # determine the mutual information\n",
        "# mutual_info = mutual_info_classif(X_train, y_train)\n",
        "# mutual_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Q8vNEyWHzLfQ",
        "outputId": "e6e646a9-aef2-4f4c-8e99-17291d0c3639"
      },
      "outputs": [],
      "source": [
        "# #ARRANGING THEM IN DESCENDIG VALUES\n",
        "\n",
        "# mutual_info = pd.Series(mutual_info)\n",
        "# mutual_info.index = X_train.columns\n",
        "# mutual_info.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1kqK1KsgzPH-",
        "outputId": "0e1df2d1-c03a-42e4-be1a-48633bc00e35"
      },
      "outputs": [],
      "source": [
        "# #PLOT :)\n",
        "\n",
        "# mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))\n",
        "\n",
        "# #FEATURES HAVING MUTUAL INFO VALUE -> 0 HAVE VERY LITTLE IMPACT ON THE TARGET VARIABLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #IMPLEMENTING BACKWARD WRAPPER SELECTION TECHNIQUE\n",
        "\n",
        "# from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "# dt = DecisionTreeClassifier()\n",
        "# dt.fit(X_train, y_train)\n",
        "\n",
        "# sfs_accuracy_score_forward = SFS(dt,\n",
        "#            k_features=17,\n",
        "#            forward=True,\n",
        "#            floating=False,\n",
        "#            scoring='accuracy',\n",
        "#            )\n",
        "# sfs_accuracy_score_forward = sfs_accuracy_score_forward.fit(X_train, y_train)\n",
        "\n",
        "# #CREATED DATAFRAME\n",
        "\n",
        "# metric_df_accuracy_forward = pd.DataFrame.from_dict(sfs_accuracy_score_forward.get_metric_dict()).T\n",
        "# metric_df_accuracy_forward['observations'] = 404\n",
        "# metric_df_accuracy_forward['num_features'] = metric_df_accuracy_forward['feature_idx'].apply(lambda x: len(x))\n",
        "\n",
        "# #CREATING LIST IN DESCENDING ORDER TO REPRESENT AVG_SCORES\n",
        "\n",
        "# features_score_forward = metric_df_accuracy_forward.groupby('feature_idx').agg({'avg_score': 'mean'}).reset_index()\n",
        "# features_score_forward.sort_values('avg_score', ascending=False, inplace=True)\n",
        "# features_score_forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #PRINTING MATRIX, AVG_SCORE AT EACH WRAPPER STEP\n",
        "\n",
        "# print(\"ACCURACY MATRIX : \")\n",
        "# print(metric_df_accuracy_forward)\n",
        "# accuracy_scores = metric_df_accuracy_forward['avg_score']\n",
        "# print(\"Accuracy scores at each step:\")\n",
        "# print(accuracy_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #PLOTTING GRAPH FOR BETTER UNDERSTANDING\n",
        "\n",
        "# from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
        "\n",
        "# fig1 = plot_sfs(sfs_accuracy_score_forward.get_metric_dict(), kind='std_dev')\n",
        "# plt.title('WRAPPER SELECTION :)')\n",
        "# plt.grid()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        " #IMPORTING LIBRARIES TO USE DIFFERENT MODELS FOR MODELLING PROCESS\n",
        "# !pip install bayesian-optimization\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from statistics import mean, stdev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "#FUNCTION FOR BAYESIAN OPTIMIZER FOR RANDOM FOREST\n",
        "\n",
        "def rf_cv(n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features):\n",
        "    # Convert parameters to integer\n",
        "    n_estimators = int(n_estimators)\n",
        "    max_depth = int(max_depth) if max_depth != None else None\n",
        "    min_samples_split = int(min_samples_split)\n",
        "    min_samples_leaf = int(min_samples_leaf)\n",
        "    max_features = max_features if max_features != 'auto' else None  # 'auto' is deprecated in newer versions\n",
        "    \n",
        "    # Create a RandomForestClassifier instance with the given parameters\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        random_state=60\n",
        "    )\n",
        "    \n",
        "    # Perform cross-validation and return the mean score\n",
        "    return np.max(cross_val_score(rf, X_train_scaled, y_train, cv=5, n_jobs=-1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Suyash Shringi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Best Score: 0.8571366750811397\n",
            "Random Forest Classifier with Best Hyperparameters:\n",
            "Train Result:\n",
            "\n",
            "Accuracy Score: 100.00%\n",
            "Confusion Matrix: \n",
            "[[989   0]\n",
            " [  0 187]]\n",
            "\n",
            "Mean Squared Error: 0.00\n",
            "\n",
            "Classification Report: \n",
            "               0      1  accuracy  macro avg  weighted avg\n",
            "precision    1.0    1.0       1.0        1.0           1.0\n",
            "recall       1.0    1.0       1.0        1.0           1.0\n",
            "f1-score     1.0    1.0       1.0        1.0           1.0\n",
            "support    989.0  187.0       1.0     1176.0        1176.0\n",
            "\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "Test Result:\n",
            "\n",
            "Accuracy Score: 85.03%\n",
            "Confusion Matrix: \n",
            "[[242   2]\n",
            " [ 42   8]]\n",
            "\n",
            "Mean Squared Error: 0.15\n",
            "\n",
            "Classification Report: \n",
            "                    0          1  accuracy   macro avg  weighted avg\n",
            "precision    0.852113   0.800000   0.85034    0.826056      0.843250\n",
            "recall       0.991803   0.160000   0.85034    0.575902      0.850340\n",
            "f1-score     0.916667   0.266667   0.85034    0.591667      0.806122\n",
            "support    244.000000  50.000000   0.85034  294.000000    294.000000\n",
            "\n",
            "Precision: 0.84\n",
            "Recall: 0.85\n"
          ]
        }
      ],
      "source": [
        "#HYPER-PARAMETER TUING OF RANDOM FOREST CLASSIFIER USING GRID_SEARCHcv\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50,100],\n",
        "    'max_depth': [None,1,2],\n",
        "    'min_samples_split': [2,3,4],\n",
        "    'min_samples_leaf': [1,2,3,4,5,6],\n",
        "    'max_features': ['auto' ,'sqrt']\n",
        "}\n",
        "# Create a RandomForestClassifier instance\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Create a GridSearchCV instance\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the GridSearchCV instance\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_rf = grid_search.best_estimator_\n",
        "print(\"Random Forest Classifier with Best Hyperparameters:\")\n",
        "print_score1(best_rf, X_train_scaled, y_train, X_test_scaled, y_test, train=True)\n",
        "print_score1(best_rf, X_train_scaled, y_train, X_test_scaled, y_test, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | max_depth | max_fe... | min_sa... | min_sa... | n_esti... |\n",
            "-------------------------------------------------------------------------------------\n",
            "| \u001b[30m1         | \u001b[30m0.8723    | \u001b[30m9.989     | \u001b[30m0.9547    | \u001b[30m11.25     | \u001b[30m9.783     | \u001b[30m65.6      |\n",
            "| \u001b[30m2         | \u001b[30m0.8468    | \u001b[30m4.744     | \u001b[30m0.1522    | \u001b[30m13.13     | \u001b[30m9.814     | \u001b[30m120.8     |\n",
            "| \u001b[30m3         | \u001b[30m0.8426    | \u001b[30m1.494     | \u001b[30m0.9719    | \u001b[30m12.65     | \u001b[30m4.76      | \u001b[30m68.18     |\n",
            "| \u001b[30m4         | \u001b[30m0.8638    | \u001b[30m5.402     | \u001b[30m0.3735    | \u001b[30m8.347     | \u001b[30m7.615     | \u001b[30m79.12     |\n",
            "| \u001b[30m5         | \u001b[30m0.8681    | \u001b[30m15.68     | \u001b[30m0.2254    | \u001b[30m5.09      | \u001b[30m6.763     | \u001b[30m95.61     |\n",
            "| \u001b[30m6         | \u001b[30m0.8602    | \u001b[30m19.84     | \u001b[30m0.2795    | \u001b[30m8.199     | \u001b[30m9.701     | \u001b[30m54.65     |\n",
            "| \u001b[30m7         | \u001b[30m0.8723    | \u001b[30m15.58     | \u001b[30m0.2533    | \u001b[30m1.911     | \u001b[30m14.34     | \u001b[30m146.6     |\n",
            "| \u001b[30m8         | \u001b[30m0.8723    | \u001b[30m20.4      | \u001b[30m0.3738    | \u001b[30m2.367     | \u001b[30m10.9      | \u001b[30m94.02     |\n",
            "| \u001b[30m9         | \u001b[30m0.8644    | \u001b[30m3.929     | \u001b[30m0.5452    | \u001b[30m1.481     | \u001b[30m13.82     | \u001b[30m75.88     |\n",
            "| \u001b[30m10        | \u001b[30m0.8596    | \u001b[30m16.9      | \u001b[30m0.3802    | \u001b[30m8.281     | \u001b[30m9.107     | \u001b[30m68.49     |\n",
            "| \u001b[30m11        | \u001b[30m0.8723    | \u001b[30m9.079     | \u001b[30m0.747     | \u001b[30m11.74     | \u001b[30m10.43     | \u001b[30m65.21     |\n",
            "| \u001b[35m12        | \u001b[35m0.8809    | \u001b[35m14.93     | \u001b[35m0.8996    | \u001b[35m1.056     | \u001b[35m13.89     | \u001b[35m91.85     |\n",
            "| \u001b[30m13        | \u001b[30m0.8723    | \u001b[30m17.15     | \u001b[30m0.999     | \u001b[30m1.0       | \u001b[30m15.0      | \u001b[30m86.42     |\n",
            "| \u001b[30m14        | \u001b[30m0.8723    | \u001b[30m8.616     | \u001b[30m0.2962    | \u001b[30m1.0       | \u001b[30m15.0      | \u001b[30m92.69     |\n",
            "| \u001b[30m15        | \u001b[30m0.8638    | \u001b[30m14.88     | \u001b[30m0.999     | \u001b[30m7.276     | \u001b[30m15.0      | \u001b[30m91.99     |\n",
            "| \u001b[30m16        | \u001b[30m0.8766    | \u001b[30m13.27     | \u001b[30m0.999     | \u001b[30m1.0       | \u001b[30m9.356     | \u001b[30m89.36     |\n",
            "| \u001b[30m17        | \u001b[30m0.8766    | \u001b[30m15.07     | \u001b[30m0.999     | \u001b[30m1.0       | \u001b[30m15.0      | \u001b[30m98.87     |\n",
            "| \u001b[30m18        | \u001b[30m0.8766    | \u001b[30m23.36     | \u001b[30m0.999     | \u001b[30m1.0       | \u001b[30m15.0      | \u001b[30m105.3     |\n",
            "| \u001b[30m19        | \u001b[30m0.8766    | \u001b[30m24.68     | \u001b[30m0.999     | \u001b[30m1.0       | \u001b[30m6.568     | \u001b[30m109.0     |\n",
            "| \u001b[30m20        | \u001b[30m0.8723    | \u001b[30m25.0      | \u001b[30m0.999     | \u001b[30m1.0       | \u001b[30m14.09     | \u001b[30m115.0     |\n",
            "| \u001b[30m21        | \u001b[30m0.8766    | \u001b[30m16.27     | \u001b[30m0.999     | \u001b[30m1.0       | \u001b[30m9.965     | \u001b[30m108.4     |\n",
            "| \u001b[30m22        | \u001b[30m0.8681    | \u001b[30m24.98     | \u001b[30m0.999     | \u001b[30m10.03     | \u001b[30m10.47     | \u001b[30m108.4     |\n",
            "| \u001b[30m23        | \u001b[30m0.8644    | \u001b[30m19.32     | \u001b[30m0.1       | \u001b[30m1.0       | \u001b[30m2.0       | \u001b[30m115.3     |\n",
            "| \u001b[30m24        | \u001b[30m0.8723    | \u001b[30m25.0      | \u001b[30m0.999     | \u001b[30m1.0       | \u001b[30m2.59      | \u001b[30m101.4     |\n",
            "| \u001b[30m25        | \u001b[30m0.8766    | \u001b[30m23.02     | \u001b[30m0.7937    | \u001b[30m1.011     | \u001b[30m4.042     | \u001b[30m149.5     |\n",
            "| \u001b[30m26        | \u001b[30m0.8638    | \u001b[30m22.27     | \u001b[30m0.9829    | \u001b[30m10.2      | \u001b[30m6.462     | \u001b[30m148.0     |\n",
            "| \u001b[30m27        | \u001b[30m0.8766    | \u001b[30m14.05     | \u001b[30m0.2247    | \u001b[30m1.0       | \u001b[30m2.0       | \u001b[30m150.0     |\n",
            "| \u001b[30m28        | \u001b[30m0.8602    | \u001b[30m17.78     | \u001b[30m0.1       | \u001b[30m1.0       | \u001b[30m2.0       | \u001b[30m141.5     |\n",
            "| \u001b[30m29        | \u001b[30m0.8723    | \u001b[30m7.074     | \u001b[30m0.999     | \u001b[30m1.0       | \u001b[30m7.541     | \u001b[30m150.0     |\n",
            "| \u001b[30m30        | \u001b[30m0.8809    | \u001b[30m9.601     | \u001b[30m0.999     | \u001b[30m1.0       | \u001b[30m15.0      | \u001b[30m106.5     |\n",
            "| \u001b[30m31        | \u001b[30m0.8681    | \u001b[30m6.974     | \u001b[30m0.999     | \u001b[30m1.0       | \u001b[30m9.097     | \u001b[30m103.9     |\n",
            "| \u001b[30m32        | \u001b[30m0.8468    | \u001b[30m14.28     | \u001b[30m0.1       | \u001b[30m4.96      | \u001b[30m15.0      | \u001b[30m107.5     |\n",
            "| \u001b[30m33        | \u001b[30m0.8766    | \u001b[30m14.28     | \u001b[30m0.927     | \u001b[30m1.0       | \u001b[30m12.52     | \u001b[30m94.79     |\n",
            "| \u001b[30m34        | \u001b[30m0.8511    | \u001b[30m16.74     | \u001b[30m0.1       | \u001b[30m1.0       | \u001b[30m11.29     | \u001b[30m90.59     |\n",
            "| \u001b[30m35        | \u001b[30m0.8766    | \u001b[30m14.29     | \u001b[30m0.999     | \u001b[30m1.028     | \u001b[30m13.89     | \u001b[30m93.25     |\n",
            "| \u001b[30m36        | \u001b[30m0.8638    | \u001b[30m13.69     | \u001b[30m0.999     | \u001b[30m1.0       | \u001b[30m14.95     | \u001b[30m91.12     |\n",
            "| \u001b[30m37        | \u001b[30m0.8723    | \u001b[30m15.87     | \u001b[30m0.999     | \u001b[30m1.209     | \u001b[30m13.8      | \u001b[30m92.84     |\n",
            "| \u001b[30m38        | \u001b[30m0.8809    | \u001b[30m14.32     | \u001b[30m0.999     | \u001b[30m1.0       | \u001b[30m12.68     | \u001b[30m92.26     |\n",
            "| \u001b[30m39        | \u001b[30m0.8681    | \u001b[30m8.444     | \u001b[30m0.8136    | \u001b[30m1.0       | \u001b[30m14.47     | \u001b[30m105.5     |\n",
            "| \u001b[30m40        | \u001b[30m0.8517    | \u001b[30m14.38     | \u001b[30m0.1       | \u001b[30m2.002     | \u001b[30m13.21     | \u001b[30m92.31     |\n",
            "=====================================================================================\n",
            "Best Parameters: {'max_depth': 14.93073251590027, 'max_features': 0.8995720107051951, 'min_samples_leaf': 1.0556095611333263, 'min_samples_split': 13.887422815054146, 'n_estimators': 91.84768233638677}\n",
            "Random Forest Classifier with Best Hyperparameters:\n",
            "Train Result:\n",
            "\n",
            "Confusion Matrix: \n",
            "[[986   3]\n",
            " [ 66 121]]\n",
            "\n",
            "Mean Squared Error: 0.06\n",
            "\n",
            "Classification Report: \n",
            "                    0           1  accuracy    macro avg  weighted avg\n",
            "precision    0.937262    0.975806  0.941327     0.956534      0.943391\n",
            "recall       0.996967    0.647059  0.941327     0.822013      0.941327\n",
            "f1-score     0.966193    0.778135  0.941327     0.872164      0.936289\n",
            "support    989.000000  187.000000  0.941327  1176.000000   1176.000000\n",
            "\n",
            "Precision: 0.94\n",
            "Recall: 0.94\n",
            "Accuracy Score: 94.13%\n",
            "Test Result:\n",
            "\n",
            "Confusion Matrix: \n",
            "[[239   5]\n",
            " [ 43   7]]\n",
            "\n",
            "Mean Squared Error: 0.16\n",
            "\n",
            "Classification Report: \n",
            "                    0          1  accuracy   macro avg  weighted avg\n",
            "precision    0.847518   0.583333  0.836735    0.715426      0.802588\n",
            "recall       0.979508   0.140000  0.836735    0.559754      0.836735\n",
            "f1-score     0.908745   0.225806  0.836735    0.567276      0.792599\n",
            "support    244.000000  50.000000  0.836735  294.000000    294.000000\n",
            "\n",
            "Precision: 0.80\n",
            "Recall: 0.84\n",
            "Accuracy Score: 88.09%\n"
          ]
        }
      ],
      "source": [
        "#HYPER-PARAMETER TUNING OF RANDOM FOREST CLASSIFIER USING BAYESIAN-OPTIMIZATION\n",
        "\n",
        "param_bounds = {\n",
        "    'n_estimators': (50,150),\n",
        "    'max_depth': (1, 25),\n",
        "    'min_samples_split': (2, 15),\n",
        "    'min_samples_leaf': (1, 15),\n",
        "    'max_features': (0.1, 0.999)  # max_features should be a fraction of features to consider\n",
        "}\n",
        "\n",
        "\n",
        "# Create a BayesianOptimization instance\n",
        "optimizer = BayesianOptimization(\n",
        "    f=rf_cv,\n",
        "    pbounds=param_bounds,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Perform the optimization\n",
        "optimizer.maximize(init_points=10, n_iter=30)\n",
        "best_val = optimizer.max['target']\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best Parameters:\", optimizer.max['params'])\n",
        "\n",
        "# Create the best RandomForestClassifier with the optimized parameters\n",
        "best_params = optimizer.max['params']\n",
        "best_rf = RandomForestClassifier(\n",
        "    n_estimators=int(best_params['n_estimators']),\n",
        "    max_depth=int(best_params['max_depth']) if best_params['max_depth'] != None else None,\n",
        "    min_samples_split=int(best_params['min_samples_split']),\n",
        "    min_samples_leaf=int(best_params['min_samples_leaf']),\n",
        "    max_features=best_params['max_features'] if best_params['max_features'] != 'auto' else None,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model on the training data\n",
        "best_rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "print(\"Random Forest Classifier with Best Hyperparameters:\")\n",
        "print_score(best_rf, X_train_scaled, y_train, X_test_scaled, y_test, train=True) \n",
        "print_score(best_rf, X_train_scaled, y_train, X_test_scaled, y_test, train=False)\n",
        "print(f\"Accuracy Score: {best_val*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "#FUNCTION FOR BAYESIAN OPTIMIZATION FOR THE XGBOOST MODEL\n",
        "\n",
        "def xgb_evaluate(n_estimators, max_depth, learning_rate, subsample, colsample_bytree):\n",
        "    # Convert float parameters to integers where needed\n",
        "    n_estimators = int(n_estimators)\n",
        "    max_depth = int(max_depth)\n",
        "    \n",
        "    # Create the classifier with given parameters\n",
        "    xgb = XGBClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        learning_rate=learning_rate,\n",
        "        subsample=subsample,\n",
        "        colsample_bytree=colsample_bytree,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    # Perform cross-validation and return the mean score\n",
        "    cv_scores = cross_val_score(xgb, X_train_scaled, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "    return cv_scores.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
            "Best Parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.35, 'max_depth': 2, 'n_estimators': 50, 'subsample': 0.8}\n",
            "Best Score: 0.8733032816444284\n",
            "Random Forest Classifier with Best Hyperparameters:\n",
            "Train Result:\n",
            "\n",
            "Accuracy Score: 90.73%\n",
            "Confusion Matrix: \n",
            "[[976  13]\n",
            " [ 96  91]]\n",
            "\n",
            "Mean Squared Error: 0.09\n",
            "\n",
            "Classification Report: \n",
            "                    0           1  accuracy    macro avg  weighted avg\n",
            "precision    0.910448    0.875000  0.907313     0.892724      0.904811\n",
            "recall       0.986855    0.486631  0.907313     0.736743      0.907313\n",
            "f1-score     0.947113    0.625430  0.907313     0.786271      0.895961\n",
            "support    989.000000  187.000000  0.907313  1176.000000   1176.000000\n",
            "\n",
            "Precision: 0.90\n",
            "Recall: 0.91\n",
            "Test Result:\n",
            "\n",
            "Accuracy Score: 85.03%\n",
            "Confusion Matrix: \n",
            "[[237   7]\n",
            " [ 37  13]]\n",
            "\n",
            "Mean Squared Error: 0.15\n",
            "\n",
            "Classification Report: \n",
            "                    0          1  accuracy   macro avg  weighted avg\n",
            "precision    0.864964   0.650000   0.85034    0.757482      0.828405\n",
            "recall       0.971311   0.260000   0.85034    0.615656      0.850340\n",
            "f1-score     0.915058   0.371429   0.85034    0.643243      0.822604\n",
            "support    244.000000  50.000000   0.85034  294.000000    294.000000\n",
            "\n",
            "Precision: 0.83\n",
            "Recall: 0.85\n"
          ]
        }
      ],
      "source": [
        "#HYPER-PARAMETER TUNING FOR  XGboost CLASSIFIER USING GRID_SEARCH CV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50],\n",
        "    'max_depth': [2,9],\n",
        "    'learning_rate': [0.25,0.3, 0.35],\n",
        "    'subsample': [0.75, 0.8,0.85],\n",
        "    'colsample_bytree': [0.5 ,0.6, 0.7]\n",
        "}\n",
        "# Create a RandomForestClassifier instance\n",
        "xg = XGBClassifier(random_state=42)\n",
        "\n",
        "# Create a GridSearchCV instance\n",
        "grid_search = GridSearchCV(estimator=xg, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the GridSearchCV instance\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_xg = grid_search.best_estimator_\n",
        "print(\"Random Forest Classifier with Best Hyperparameters:\")\n",
        "print_score1(best_xg, X_train_scaled, y_train, X_test_scaled, y_test, train=True)\n",
        "print_score1(best_xg, X_train_scaled, y_train, X_test_scaled, y_test, train=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | colsam... | learni... | max_depth | n_esti... | subsample |\n",
            "-------------------------------------------------------------------------------------\n",
            "| \u001b[30m1         | \u001b[30m0.8298    | \u001b[30m0.4247    | \u001b[30m0.5234    | \u001b[30m15.18     | \u001b[30m199.7     | \u001b[30m0.2248    |\n",
            "| \u001b[35m2         | \u001b[35m0.8644    | \u001b[35m0.2936    | \u001b[35m0.04137   | \u001b[35m17.59     | \u001b[35m200.3     | \u001b[35m0.6665    |\n",
            "| \u001b[30m3         | \u001b[30m0.822     | \u001b[30m0.2124    | \u001b[30m0.5338    | \u001b[30m16.98     | \u001b[30m103.1     | \u001b[30m0.2455    |\n",
            "| \u001b[30m4         | \u001b[30m0.8596    | \u001b[30m0.31      | \u001b[30m0.1743    | \u001b[30m11.45     | \u001b[30m158.0     | \u001b[30m0.333     |\n",
            "| \u001b[35m5         | \u001b[35m0.8809    | \u001b[35m0.5671    | \u001b[35m0.08533   | \u001b[35m7.259     | \u001b[35m141.6     | \u001b[35m0.4649    |\n",
            "| \u001b[30m6         | \u001b[30m0.8766    | \u001b[30m0.5882    | \u001b[30m0.05942   | \u001b[30m7.282     | \u001b[30m141.6     | \u001b[30m0.5024    |\n",
            "| \u001b[30m7         | \u001b[30m0.8553    | \u001b[30m0.3591    | \u001b[30m0.3418    | \u001b[30m7.032     | \u001b[30m141.9     | \u001b[30m0.279     |\n",
            "| \u001b[30m8         | \u001b[30m0.8596    | \u001b[30m0.2963    | \u001b[30m0.4994    | \u001b[30m19.2      | \u001b[30m116.2     | \u001b[30m0.5934    |\n",
            "| \u001b[30m9         | \u001b[30m0.8681    | \u001b[30m0.5984    | \u001b[30m0.04513   | \u001b[30m7.293     | \u001b[30m141.5     | \u001b[30m0.1462    |\n",
            "| \u001b[30m10        | \u001b[30m0.8644    | \u001b[30m0.5291    | \u001b[30m0.02445   | \u001b[30m6.939     | \u001b[30m141.5     | \u001b[30m0.4446    |\n",
            "| \u001b[30m11        | \u001b[30m0.8638    | \u001b[30m0.5274    | \u001b[30m0.2412    | \u001b[30m7.49      | \u001b[30m141.8     | \u001b[30m0.4458    |\n",
            "| \u001b[30m12        | \u001b[30m0.8681    | \u001b[30m0.2663    | \u001b[30m0.03112   | \u001b[30m7.291     | \u001b[30m141.6     | \u001b[30m0.4211    |\n",
            "| \u001b[30m13        | \u001b[30m0.8511    | \u001b[30m0.5963    | \u001b[30m0.3557    | \u001b[30m7.216     | \u001b[30m141.4     | \u001b[30m0.4233    |\n",
            "| \u001b[30m14        | \u001b[30m0.8638    | \u001b[30m0.5706    | \u001b[30m0.08882   | \u001b[30m7.262     | \u001b[30m141.6     | \u001b[30m0.4684    |\n",
            "| \u001b[30m15        | \u001b[30m0.8638    | \u001b[30m0.2449    | \u001b[30m0.4454    | \u001b[30m15.05     | \u001b[30m211.2     | \u001b[30m0.3842    |\n",
            "| \u001b[30m16        | \u001b[30m0.8553    | \u001b[30m0.7653    | \u001b[30m0.4524    | \u001b[30m4.394     | \u001b[30m191.0     | \u001b[30m0.2075    |\n",
            "| \u001b[30m17        | \u001b[30m0.8681    | \u001b[30m0.6452    | \u001b[30m0.1585    | \u001b[30m5.738     | \u001b[30m288.8     | \u001b[30m0.7669    |\n",
            "| \u001b[30m18        | \u001b[30m0.8596    | \u001b[30m0.7573    | \u001b[30m0.3802    | \u001b[30m10.48     | \u001b[30m291.1     | \u001b[30m0.8501    |\n",
            "| \u001b[30m19        | \u001b[30m0.8602    | \u001b[30m0.623     | \u001b[30m0.1013    | \u001b[30m5.595     | \u001b[30m128.0     | \u001b[30m0.2687    |\n",
            "| \u001b[30m20        | \u001b[30m0.8596    | \u001b[30m0.7541    | \u001b[30m0.2862    | \u001b[30m13.77     | \u001b[30m181.4     | \u001b[30m0.8428    |\n",
            "| \u001b[30m21        | \u001b[30m0.8723    | \u001b[30m0.734     | \u001b[30m0.04683   | \u001b[30m9.598     | \u001b[30m258.1     | \u001b[30m0.3804    |\n",
            "| \u001b[30m22        | \u001b[30m0.8596    | \u001b[30m0.4296    | \u001b[30m0.538     | \u001b[30m3.057     | \u001b[30m259.9     | \u001b[30m0.6394    |\n",
            "| \u001b[30m23        | \u001b[30m0.8213    | \u001b[30m0.4727    | \u001b[30m0.5345    | \u001b[30m4.127     | \u001b[30m80.74     | \u001b[30m0.2739    |\n",
            "| \u001b[30m24        | \u001b[30m0.8638    | \u001b[30m0.6605    | \u001b[30m0.1154    | \u001b[30m3.713     | \u001b[30m274.8     | \u001b[30m0.1204    |\n",
            "| \u001b[30m25        | \u001b[30m0.8596    | \u001b[30m0.7507    | \u001b[30m0.2183    | \u001b[30m13.91     | \u001b[30m121.6     | \u001b[30m0.2242    |\n",
            "| \u001b[30m26        | \u001b[30m0.8596    | \u001b[30m0.4234    | \u001b[30m0.2266    | \u001b[30m18.53     | \u001b[30m232.4     | \u001b[30m0.1975    |\n",
            "| \u001b[30m27        | \u001b[30m0.8723    | \u001b[30m0.5398    | \u001b[30m0.2854    | \u001b[30m11.3      | \u001b[30m273.3     | \u001b[30m0.8745    |\n",
            "| \u001b[30m28        | \u001b[30m0.8681    | \u001b[30m0.4741    | \u001b[30m0.5328    | \u001b[30m14.64     | \u001b[30m151.0     | \u001b[30m0.8489    |\n",
            "| \u001b[30m29        | \u001b[30m0.8426    | \u001b[30m0.524     | \u001b[30m0.4118    | \u001b[30m5.25      | \u001b[30m241.2     | \u001b[30m0.343     |\n",
            "| \u001b[30m30        | \u001b[30m0.8723    | \u001b[30m0.2044    | \u001b[30m0.08303   | \u001b[30m14.36     | \u001b[30m273.0     | \u001b[30m0.301     |\n",
            "| \u001b[30m31        | \u001b[30m0.8638    | \u001b[30m0.6864    | \u001b[30m0.2566    | \u001b[30m8.484     | \u001b[30m141.7     | \u001b[30m0.5378    |\n",
            "| \u001b[30m32        | \u001b[30m0.8638    | \u001b[30m0.3902    | \u001b[30m0.1371    | \u001b[30m7.081     | \u001b[30m166.7     | \u001b[30m0.6163    |\n",
            "| \u001b[30m33        | \u001b[30m0.8553    | \u001b[30m0.2663    | \u001b[30m0.205     | \u001b[30m4.079     | \u001b[30m126.2     | \u001b[30m0.138     |\n",
            "| \u001b[30m34        | \u001b[30m0.8681    | \u001b[30m0.2246    | \u001b[30m0.0701    | \u001b[30m6.737     | \u001b[30m195.8     | \u001b[30m0.2629    |\n",
            "| \u001b[30m35        | \u001b[30m0.8559    | \u001b[30m0.6803    | \u001b[30m0.03652   | \u001b[30m19.79     | \u001b[30m73.73     | \u001b[30m0.7441    |\n",
            "| \u001b[30m36        | \u001b[30m0.8681    | \u001b[30m0.5564    | \u001b[30m0.09537   | \u001b[30m14.17     | \u001b[30m123.4     | \u001b[30m0.6243    |\n",
            "| \u001b[30m37        | \u001b[30m0.8596    | \u001b[30m0.2885    | \u001b[30m0.3289    | \u001b[30m6.631     | \u001b[30m214.2     | \u001b[30m0.8186    |\n",
            "| \u001b[30m38        | \u001b[30m0.8681    | \u001b[30m0.5086    | \u001b[30m0.2463    | \u001b[30m2.877     | \u001b[30m154.4     | \u001b[30m0.1006    |\n",
            "| \u001b[30m39        | \u001b[30m0.8723    | \u001b[30m0.4214    | \u001b[30m0.07907   | \u001b[30m8.907     | \u001b[30m221.0     | \u001b[30m0.3476    |\n",
            "| \u001b[30m40        | \u001b[30m0.8766    | \u001b[30m0.7066    | \u001b[30m0.1849    | \u001b[30m3.216     | \u001b[30m92.57     | \u001b[30m0.2063    |\n",
            "| \u001b[30m41        | \u001b[30m0.8638    | \u001b[30m0.5525    | \u001b[30m0.3462    | \u001b[30m15.2      | \u001b[30m225.5     | \u001b[30m0.3143    |\n",
            "| \u001b[30m42        | \u001b[30m0.8596    | \u001b[30m0.7127    | \u001b[30m0.4784    | \u001b[30m12.6      | \u001b[30m218.4     | \u001b[30m0.4815    |\n",
            "| \u001b[30m43        | \u001b[30m0.8766    | \u001b[30m0.671     | \u001b[30m0.03995   | \u001b[30m3.603     | \u001b[30m264.2     | \u001b[30m0.3183    |\n",
            "| \u001b[30m44        | \u001b[30m0.8766    | \u001b[30m0.2412    | \u001b[30m0.2845    | \u001b[30m2.618     | \u001b[30m80.68     | \u001b[30m0.717     |\n",
            "| \u001b[30m45        | \u001b[30m0.8723    | \u001b[30m0.5343    | \u001b[30m0.05257   | \u001b[30m7.226     | \u001b[30m141.6     | \u001b[30m0.4321    |\n",
            "=====================================================================================\n",
            "Best Parameters: {'colsample_bytree': 0.5671117368334277, 'learning_rate': 0.08532668475210259, 'max_depth': 7, 'n_estimators': 141, 'subsample': 0.46485598737362877}\n",
            "XG BOOST Classifier with Best Hyperparameters:\n",
            "Train Result:\n",
            "\n",
            "Confusion Matrix: \n",
            "[[989   0]\n",
            " [ 14 173]]\n",
            "\n",
            "Mean Squared Error: 0.01\n",
            "\n",
            "Classification Report: \n",
            "                    0           1  accuracy    macro avg  weighted avg\n",
            "precision    0.986042    1.000000  0.988095     0.993021      0.988261\n",
            "recall       1.000000    0.925134  0.988095     0.962567      0.988095\n",
            "f1-score     0.992972    0.961111  0.988095     0.977041      0.987906\n",
            "support    989.000000  187.000000  0.988095  1176.000000   1176.000000\n",
            "\n",
            "Precision: 0.99\n",
            "Recall: 0.99\n",
            "Accuracy Score: 98.81%\n",
            "Test Result:\n",
            "\n",
            "Confusion Matrix: \n",
            "[[239   5]\n",
            " [ 39  11]]\n",
            "\n",
            "Mean Squared Error: 0.15\n",
            "\n",
            "Classification Report: \n",
            "                    0          1  accuracy   macro avg  weighted avg\n",
            "precision    0.859712   0.687500   0.85034    0.773606      0.830424\n",
            "recall       0.979508   0.220000   0.85034    0.599754      0.850340\n",
            "f1-score     0.915709   0.333333   0.85034    0.624521      0.816665\n",
            "support    244.000000  50.000000   0.85034  294.000000    294.000000\n",
            "\n",
            "Precision: 0.83\n",
            "Recall: 0.85\n",
            "Accuracy Score: 88.09%\n"
          ]
        }
      ],
      "source": [
        "#HYPER-PARAMETER TUNING FOR XGboost CLASSIFIER USING BAYESIAN-OPTIMIZATION\n",
        "\n",
        "param_bounds = {\n",
        "    'n_estimators': (50, 300),  # Only one value, but necessary to define the bounds\n",
        "    'max_depth': (2,20),\n",
        "    'learning_rate': (0.01, 0.55),\n",
        "    'subsample': (0.1, 0.9),\n",
        "    'colsample_bytree': (0.2, 0.8)\n",
        "}\n",
        "\n",
        "# Initialize Bayesian Optimization\n",
        "optimizer = BayesianOptimization(f=xgb_evaluate, pbounds=param_bounds, random_state=42, verbose=2)\n",
        "\n",
        "# Run the optimization\n",
        "optimizer.maximize(init_points=5, n_iter=40)\n",
        "best_val = optimizer.max['target']\n",
        "\n",
        "\n",
        "# Extract the best parameters\n",
        "best_params = optimizer.max['params']\n",
        "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
        "best_params['max_depth'] = int(best_params['max_depth'])\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Train the final model with the best parameters\n",
        "best_xg = XGBClassifier(**best_params, random_state=42)\n",
        "best_xg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the best model\n",
        "print(\"XG BOOST Classifier with Best Hyperparameters:\")\n",
        "print_score(best_xg, X_train, y_train, X_test, y_test, train=True)\n",
        "print_score(best_xg, X_train, y_train, X_test, y_test, train=False)\n",
        "print(f\"Accuracy Score: {best_val*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle as pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "pkl.dump(best_rf,open('employee1.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "pkl.dump(scaler,open('scaler.pkl','wb'))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
